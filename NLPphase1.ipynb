{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20dGaTxdE9Jh"
      },
      "outputs": [],
      "source": [
        "#-----NLP Phase 1: Text Processing-------\n",
        "\n",
        "#text -> cleaning -> tokenization -> stopword removal -> steming/lammetization -> vectorization -> ML\n",
        "\n",
        "text=\"I am learning python programming. Python is fast. Isn't it?\"\n",
        "print(\"Original Text :\",text)\n",
        "print()\n",
        "\n",
        "#step 1 - lower case\n",
        "text=text.lower()\n",
        "print(\"Step 1: \",text)\n",
        "\n",
        "#step 2 -remove punctuations- !@#$%^&*().?\n",
        "import re  #regex- regular expressions\n",
        "\n",
        "#remove everyhting except letters and spaces\n",
        "text=re.sub(r'[^\\w\\s]','',text)\n",
        "print(\"Step 2: \",text)\n",
        "\n",
        "#step 3 : tokenization [Split sentence into words]\n",
        "tokens=text.split()\n",
        "print(\"Step 3: \",tokens)\n",
        "\n",
        "#step 4 : Remove Stopwords-is,am,are...\n",
        "stopwords=[\"is\",\"am\",\"are\",\"the\",\"a\"]\n",
        "filtered_tokens=[x for x in tokens if x not in stopwords]\n",
        "print(\"Step 4: \",filtered_tokens)\n",
        "\n",
        "#step 5 : stemming as lammetization(not possible using simple python)\n",
        "def stem(word):\n",
        "  if word.endswith(\"ing\"):\n",
        "    return word[:-3]\n",
        "  if word.endswith(\"ed\"):\n",
        "    return word[:-2]\n",
        "  return word\n",
        "stemmed_words=[stem(x) for x in filtered_tokens]\n",
        "print(\"Step 5: \",stemmed_words)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
